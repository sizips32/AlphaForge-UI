"""
Dask ëŒ€ìš©ëŸ‰ ë°ì´í„° ì²˜ë¦¬ ì‹œìŠ¤í…œ
ë¶„ì‚° ì»´í“¨íŒ…ì„ ìœ„í•œ Dask í†µí•© ë° ëŒ€ìš©ëŸ‰ ë°ì´í„°ì…‹ ì²˜ë¦¬
"""

import os
import time
import warnings
from datetime import datetime
from typing import Dict, Any, List, Optional, Union, Callable, Tuple
from dataclasses import dataclass
from enum import Enum
import numpy as np
import pandas as pd
from pathlib import Path

# Dask ê´€ë ¨ imports (ì„ íƒì )
try:
    import dask
    import dask.dataframe as dd
    from dask.distributed import Client, LocalCluster, as_completed
    from dask import delayed, compute
    from dask.diagnostics import ProgressBar
    DASK_AVAILABLE = True
except ImportError:
    DASK_AVAILABLE = False
    # Fallback í´ë˜ìŠ¤ë“¤
    class MockDaskDataFrame:
        def __init__(self, df):
            self._df = df
        
        def compute(self):
            return self._df
        
        def __getattr__(self, name):
            return getattr(self._df, name)

from utils.logger import get_logger, log_performance
from utils.error_handler import handle_error, ErrorCategory, ErrorSeverity
from utils.realtime_updates import realtime_manager, UpdateType


class DaskBackend(Enum):
    """Dask ë°±ì—”ë“œ"""
    THREADS = "threads"
    PROCESSES = "processes"
    SYNCHRONOUS = "synchronous"
    DISTRIBUTED = "distributed"


class ComputeMode(Enum):
    """ê³„ì‚° ëª¨ë“œ"""
    LAZY = "lazy"
    EAGER = "eager"
    STREAMING = "streaming"


@dataclass
class DaskConfig:
    """Dask ì„¤ì •"""
    backend: DaskBackend
    n_workers: int
    threads_per_worker: int
    memory_limit: str
    chunk_size: Union[int, str]
    compute_mode: ComputeMode
    scheduler_address: Optional[str] = None
    temporary_directory: Optional[str] = None
    dashboard_address: Optional[str] = None


@dataclass
class ProcessingResult:
    """ì²˜ë¦¬ ê²°ê³¼"""
    result: Any
    processing_time: float
    memory_usage: Dict[str, Any]
    task_graph_size: int
    chunk_info: Dict[str, Any]


class DaskProcessor:
    """Dask ëŒ€ìš©ëŸ‰ ë°ì´í„° ì²˜ë¦¬ê¸°"""
    
    def __init__(self, config: Optional[DaskConfig] = None):
        """ì´ˆê¸°í™”"""
        self.logger = get_logger("dask_processor")
        
        if not DASK_AVAILABLE:
            self.logger.warning("Dask not available, falling back to pandas")
            self.use_dask = False
            return
        
        self.use_dask = True
        
        # ê¸°ë³¸ ì„¤ì •
        if config is None:
            config = DaskConfig(
                backend=DaskBackend.THREADS,
                n_workers=4,
                threads_per_worker=2,
                memory_limit="2GB",
                chunk_size="100MB",
                compute_mode=ComputeMode.LAZY
            )
        
        self.config = config
        self.client = None
        self.cluster = None
        
        # Dask ì„¤ì • ì ìš©
        self._configure_dask()
        self._setup_client()
        
        # ì„±ëŠ¥ ì¶”ì 
        self.task_history: List[Dict[str, Any]] = []
    
    def _configure_dask(self):
        """Dask ì „ì—­ ì„¤ì •"""
        # ê²½ê³  ì–µì œ
        warnings.filterwarnings('ignore', category=UserWarning, module='dask')
        
        # ê¸°ë³¸ ì„¤ì •
        dask.config.set({
            'dataframe.query-planning': False,  # ì•ˆì •ì„±ì„ ìœ„í•´ ë¹„í™œì„±í™”
            'array.slicing.split_large_chunks': True,
            'optimization.fuse': {},  # ìµœì í™” ë¹„í™œì„±í™” (ì•ˆì •ì„± ìš°ì„ )
            'distributed.worker.daemon': False
        })
        
        if self.config.temporary_directory:
            dask.config.set({'temporary-directory': self.config.temporary_directory})
    
    def _setup_client(self):
        """Dask í´ë¼ì´ì–¸íŠ¸ ì„¤ì •"""
        try:
            if self.config.backend == DaskBackend.DISTRIBUTED and self.config.scheduler_address:
                # ê¸°ì¡´ ë¶„ì‚° í´ëŸ¬ìŠ¤í„°ì— ì—°ê²°
                self.client = Client(self.config.scheduler_address)
                self.logger.info(f"Connected to distributed cluster: {self.config.scheduler_address}")
            
            elif self.config.backend == DaskBackend.DISTRIBUTED:
                # ë¡œì»¬ í´ëŸ¬ìŠ¤í„° ìƒì„±
                self.cluster = LocalCluster(
                    n_workers=self.config.n_workers,
                    threads_per_worker=self.config.threads_per_worker,
                    memory_limit=self.config.memory_limit,
                    dashboard_address=self.config.dashboard_address or ':0'
                )
                self.client = Client(self.cluster)
                self.logger.info(f"Created local cluster with {self.config.n_workers} workers")
            
            else:
                # ìŠ¤ë ˆë“œ ë˜ëŠ” í”„ë¡œì„¸ìŠ¤ ë°±ì—”ë“œ
                self.client = None
                self.logger.info(f"Using {self.config.backend.value} backend")
        
        except Exception as e:
            self.logger.error(f"Failed to setup Dask client: {e}")
            self.client = None
            self.cluster = None
    
    def load_data(
        self,
        data_source: Union[str, pd.DataFrame, List[str]],
        file_format: str = 'auto',
        **read_kwargs
    ) -> Union[dd.DataFrame, pd.DataFrame]:
        """ë°ì´í„° ë¡œë“œ"""
        if not self.use_dask:
            # Pandas fallback
            if isinstance(data_source, str):
                if data_source.endswith('.csv'):
                    return pd.read_csv(data_source, **read_kwargs)
                elif data_source.endswith('.parquet'):
                    return pd.read_parquet(data_source, **read_kwargs)
            elif isinstance(data_source, pd.DataFrame):
                return data_source
            else:
                raise ValueError("Unsupported data source for pandas fallback")
        
        try:
            if isinstance(data_source, pd.DataFrame):
                # DataFrameì„ Dask DataFrameìœ¼ë¡œ ë³€í™˜
                chunk_size = self._parse_chunk_size(len(data_source))
                ddf = dd.from_pandas(data_source, npartitions=max(1, len(data_source) // chunk_size))
                
                self.logger.info(f"Converted pandas DataFrame to Dask with {ddf.npartitions} partitions")
                return ddf
            
            elif isinstance(data_source, str):
                # íŒŒì¼ì—ì„œ ë¡œë“œ
                if file_format == 'auto':
                    file_format = Path(data_source).suffix[1:]  # í™•ì¥ìì—ì„œ . ì œê±°
                
                if file_format.lower() == 'csv':
                    ddf = dd.read_csv(
                        data_source,
                        blocksize=self.config.chunk_size,
                        **read_kwargs
                    )
                elif file_format.lower() == 'parquet':
                    ddf = dd.read_parquet(data_source, **read_kwargs)
                elif file_format.lower() == 'json':
                    ddf = dd.read_json(
                        data_source,
                        blocksize=self.config.chunk_size,
                        **read_kwargs
                    )
                else:
                    raise ValueError(f"Unsupported file format: {file_format}")
                
                self.logger.info(f"Loaded {file_format} data with {ddf.npartitions} partitions")
                return ddf
            
            elif isinstance(data_source, list):
                # ì—¬ëŸ¬ íŒŒì¼ì—ì„œ ë¡œë“œ
                if all(f.endswith('.csv') for f in data_source):
                    ddf = dd.read_csv(data_source, blocksize=self.config.chunk_size, **read_kwargs)
                elif all(f.endswith('.parquet') for f in data_source):
                    ddf = dd.read_parquet(data_source, **read_kwargs)
                else:
                    raise ValueError("Mixed file formats not supported")
                
                self.logger.info(f"Loaded {len(data_source)} files with {ddf.npartitions} partitions")
                return ddf
            
            else:
                raise ValueError(f"Unsupported data source type: {type(data_source)}")
        
        except Exception as e:
            self.logger.error(f"Failed to load data: {e}")
            handle_error(
                e,
                ErrorCategory.DATA_PROCESSING,
                ErrorSeverity.HIGH,
                context={'data_source': str(data_source)[:200]}
            )
            raise
    
    def process_data(
        self,
        ddf: Union[dd.DataFrame, pd.DataFrame],
        operations: List[Callable],
        compute_result: bool = True,
        progress_callback: Optional[Callable] = None
    ) -> ProcessingResult:
        """ë°ì´í„° ì²˜ë¦¬"""
        start_time = time.time()
        
        try:
            # Daskê°€ ì‚¬ìš© ë¶ˆê°€ëŠ¥í•œ ê²½ìš° pandasë¡œ ì²˜ë¦¬
            if not self.use_dask or isinstance(ddf, pd.DataFrame):
                return self._process_with_pandas(ddf, operations, start_time, progress_callback)
            
            self.logger.info(f"Starting Dask processing with {len(operations)} operations")
            
            # ì‘ì—… ê·¸ë˜í”„ ì •ë³´ ìˆ˜ì§‘
            initial_graph_size = len(ddf.__dask_graph__()) if hasattr(ddf, '__dask_graph__') else 0
            
            # ì—°ì‚° ì ìš© (ì§€ì—° ì‹¤í–‰)
            result_ddf = ddf
            for i, operation in enumerate(operations):
                if progress_callback:
                    progress_callback(i / len(operations), f"ì—°ì‚° {i+1}/{len(operations)} ì ìš© ì¤‘")
                
                result_ddf = operation(result_ddf)
                
                # ì‹¤ì‹œê°„ ì—…ë°ì´íŠ¸
                realtime_manager.send_progress_update(
                    task_id=f"dask_processing_{id(self)}",
                    progress=i / len(operations),
                    current_step=f"ì—°ì‚° {i+1}/{len(operations)} ì ìš© ì¤‘"
                )
            
            # ì²­í¬ ì •ë³´
            chunk_info = {
                'n_partitions': result_ddf.npartitions,
                'partition_sizes': [partition.nbytes for partition in result_ddf.to_delayed()],
                'total_size_estimate': sum(result_ddf.map_partitions(len).compute())
            }
            
            # ê³„ì‚° ì‹¤í–‰
            if compute_result and self.config.compute_mode != ComputeMode.LAZY:
                if progress_callback:
                    progress_callback(0.8, "ìµœì¢… ê³„ì‚° ì‹¤í–‰ ì¤‘")
                
                # ì§„í–‰ë¥  í‘œì‹œì™€ í•¨ê»˜ ê³„ì‚°
                with ProgressBar():
                    if self.client:
                        # ë¶„ì‚° ì‹¤í–‰
                        future = self.client.compute(result_ddf)
                        result = future.result()
                    else:
                        # ë¡œì»¬ ì‹¤í–‰
                        result = result_ddf.compute(scheduler=self.config.backend.value)
            else:
                # ì§€ì—° ì‹¤í–‰ - ê³„ì‚° ê·¸ë˜í”„ë§Œ ë°˜í™˜
                result = result_ddf
            
            # ìµœì¢… ê·¸ë˜í”„ í¬ê¸°
            final_graph_size = len(result.__dask_graph__()) if hasattr(result, '__dask_graph__') else 0
            
            processing_time = time.time() - start_time
            
            # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¶”ì •
            memory_usage = self._estimate_memory_usage(result)
            
            # ê²°ê³¼ ìƒì„±
            processing_result = ProcessingResult(
                result=result,
                processing_time=processing_time,
                memory_usage=memory_usage,
                task_graph_size=final_graph_size - initial_graph_size,
                chunk_info=chunk_info
            )
            
            # ì„±ëŠ¥ ë¡œê¹…
            log_performance(
                "dask_processing",
                processing_time,
                {
                    'n_partitions': chunk_info['n_partitions'],
                    'operations_count': len(operations),
                    'backend': self.config.backend.value,
                    'compute_mode': self.config.compute_mode.value,
                    'graph_size_increase': final_graph_size - initial_graph_size
                }
            )
            
            # ì‘ì—… íˆìŠ¤í† ë¦¬ ì €ì¥
            self.task_history.append({
                'timestamp': datetime.now(),
                'processing_time': processing_time,
                'operations_count': len(operations),
                'n_partitions': chunk_info['n_partitions'],
                'memory_usage': memory_usage
            })
            
            self.logger.info(f"Dask processing completed in {processing_time:.2f}s")
            
            return processing_result
        
        except Exception as e:
            self.logger.error(f"Dask processing failed: {e}")
            handle_error(
                e,
                ErrorCategory.DATA_PROCESSING,
                ErrorSeverity.HIGH,
                context={
                    'operations_count': len(operations),
                    'backend': self.config.backend.value
                }
            )
            raise
    
    def _process_with_pandas(
        self,
        df: pd.DataFrame,
        operations: List[Callable],
        start_time: float,
        progress_callback: Optional[Callable] = None
    ) -> ProcessingResult:
        """Pandasë¥¼ ì‚¬ìš©í•œ ëŒ€ì²´ ì²˜ë¦¬"""
        result_df = df.copy()
        
        for i, operation in enumerate(operations):
            if progress_callback:
                progress_callback(i / len(operations), f"ì—°ì‚° {i+1}/{len(operations)} ì ìš© ì¤‘")
            
            result_df = operation(result_df)
        
        processing_time = time.time() - start_time
        
        return ProcessingResult(
            result=result_df,
            processing_time=processing_time,
            memory_usage={'pandas_fallback': True},
            task_graph_size=0,
            chunk_info={'n_partitions': 1, 'fallback_mode': True}
        )
    
    def parallel_apply(
        self,
        ddf: Union[dd.DataFrame, pd.DataFrame],
        func: Callable,
        axis: int = 0,
        meta: Optional[Any] = None,
        **kwargs
    ) -> Union[dd.DataFrame, pd.DataFrame]:
        """ë³‘ë ¬ í•¨ìˆ˜ ì ìš©"""
        if not self.use_dask or isinstance(ddf, pd.DataFrame):
            return ddf.apply(func, axis=axis, **kwargs)
        
        try:
            # ë©”íƒ€ë°ì´í„° ì¶”ë¡ 
            if meta is None:
                sample = ddf.head(1)
                meta = sample.apply(func, axis=axis, **kwargs)
            
            return ddf.map_partitions(
                lambda partition: partition.apply(func, axis=axis, **kwargs),
                meta=meta
            )
        
        except Exception as e:
            self.logger.error(f"Parallel apply failed: {e}")
            raise
    
    def optimize_dataframe(self, ddf: dd.DataFrame) -> dd.DataFrame:
        """DataFrame ìµœì í™”"""
        if not self.use_dask:
            return ddf
        
        try:
            # ë¶ˆí•„ìš”í•œ ì»¬ëŸ¼ ì œê±°
            # (ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” ì»¬ëŸ¼ ì‹ë³„ ë¡œì§ í•„ìš”)
            
            # íŒŒí‹°ì…˜ ì¬ë¶„í• 
            optimal_partitions = self._calculate_optimal_partitions(ddf)
            if ddf.npartitions != optimal_partitions:
                ddf = ddf.repartition(npartitions=optimal_partitions)
                self.logger.info(f"Repartitioned to {optimal_partitions} partitions")
            
            # ì¸ë±ìŠ¤ ìµœì í™”
            if not ddf.known_divisions:
                # ì •ë ¬ëœ ì»¬ëŸ¼ì´ ìˆë‹¤ë©´ ì¸ë±ìŠ¤ë¡œ ì„¤ì •
                if 'date' in ddf.columns:
                    ddf = ddf.set_index('date', sorted=True)
                    self.logger.info("Set date column as sorted index")
            
            return ddf
        
        except Exception as e:
            self.logger.warning(f"DataFrame optimization failed: {e}")
            return ddf
    
    def save_data(
        self,
        ddf: Union[dd.DataFrame, pd.DataFrame],
        output_path: str,
        file_format: str = 'parquet',
        **write_kwargs
    ):
        """ë°ì´í„° ì €ì¥"""
        try:
            if not self.use_dask or isinstance(ddf, pd.DataFrame):
                # Pandas ì €ì¥
                if file_format == 'parquet':
                    ddf.to_parquet(output_path, **write_kwargs)
                elif file_format == 'csv':
                    ddf.to_csv(output_path, **write_kwargs)
                else:
                    raise ValueError(f"Unsupported format for pandas: {file_format}")
            else:
                # Dask ì €ì¥
                if file_format == 'parquet':
                    ddf.to_parquet(output_path, **write_kwargs)
                elif file_format == 'csv':
                    ddf.to_csv(output_path, **write_kwargs)
                elif file_format == 'hdf':
                    ddf.to_hdf(output_path, key='data', **write_kwargs)
                else:
                    raise ValueError(f"Unsupported format: {file_format}")
            
            self.logger.info(f"Data saved to {output_path} in {file_format} format")
        
        except Exception as e:
            self.logger.error(f"Failed to save data: {e}")
            handle_error(
                e,
                ErrorCategory.FILE_IO,
                ErrorSeverity.HIGH,
                context={'output_path': output_path, 'format': file_format}
            )
            raise
    
    def get_cluster_info(self) -> Dict[str, Any]:
        """í´ëŸ¬ìŠ¤í„° ì •ë³´ ì¡°íšŒ"""
        if not self.client:
            return {'status': 'no_cluster', 'backend': self.config.backend.value}
        
        try:
            scheduler_info = self.client.scheduler_info()
            return {
                'status': 'active',
                'scheduler_address': self.client.scheduler.address if self.client.scheduler else 'local',
                'n_workers': len(scheduler_info.get('workers', {})),
                'total_cores': sum(w.get('nthreads', 0) for w in scheduler_info.get('workers', {}).values()),
                'total_memory': sum(w.get('memory_limit', 0) for w in scheduler_info.get('workers', {}).values()),
                'dashboard_link': getattr(self.cluster, 'dashboard_link', None)
            }
        except Exception as e:
            self.logger.error(f"Failed to get cluster info: {e}")
            return {'status': 'error', 'error': str(e)}
    
    def _parse_chunk_size(self, total_size: int) -> int:
        """ì²­í¬ í¬ê¸° íŒŒì‹±"""
        if isinstance(self.config.chunk_size, str):
            if self.config.chunk_size.endswith('MB'):
                mb_size = int(self.config.chunk_size[:-2])
                # ëŒ€ëµì ìœ¼ë¡œ 1MBë‹¹ 1000 rowsë¡œ ì¶”ì •
                return mb_size * 1000
            elif self.config.chunk_size.endswith('GB'):
                gb_size = int(self.config.chunk_size[:-2])
                return gb_size * 1000000
            else:
                return int(self.config.chunk_size)
        else:
            return self.config.chunk_size
    
    def _calculate_optimal_partitions(self, ddf: dd.DataFrame) -> int:
        """ìµœì  íŒŒí‹°ì…˜ ìˆ˜ ê³„ì‚°"""
        # ë°ì´í„° í¬ê¸° ê¸°ë°˜
        try:
            total_size = ddf.map_partitions(len).sum().compute()
            optimal_size_per_partition = 100000  # íŒŒí‹°ì…˜ë‹¹ 10ë§Œ í–‰
            
            optimal_partitions = max(1, min(
                total_size // optimal_size_per_partition,
                self.config.n_workers * 4  # ì›Œì»¤ë‹¹ ìµœëŒ€ 4ê°œ íŒŒí‹°ì…˜
            ))
            
            return int(optimal_partitions)
        
        except Exception:
            # ê³„ì‚° ì‹¤íŒ¨ì‹œ í˜„ì¬ íŒŒí‹°ì…˜ ìˆ˜ ìœ ì§€
            return ddf.npartitions
    
    def _estimate_memory_usage(self, result: Any) -> Dict[str, Any]:
        """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¶”ì •"""
        try:
            if hasattr(result, 'memory_usage'):
                # DataFrameì˜ ê²½ìš°
                memory_bytes = result.memory_usage(deep=True).sum()
                return {
                    'estimated_mb': memory_bytes / 1024 / 1024,
                    'type': 'dataframe_computed'
                }
            elif hasattr(result, '__dask_graph__'):
                # Dask ê°ì²´ì˜ ê²½ìš°
                return {
                    'estimated_mb': 'unknown',
                    'type': 'dask_lazy',
                    'graph_size': len(result.__dask_graph__())
                }
            else:
                return {
                    'estimated_mb': 'unknown',
                    'type': type(result).__name__
                }
        except Exception:
            return {'estimated_mb': 'error', 'type': 'unknown'}
    
    def close(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        try:
            if self.client:
                self.client.close()
                self.client = None
                self.logger.info("Dask client closed")
            
            if self.cluster:
                self.cluster.close()
                self.cluster = None
                self.logger.info("Dask cluster closed")
        
        except Exception as e:
            self.logger.error(f"Error closing Dask resources: {e}")
    
    def __del__(self):
        """ì†Œë©¸ì"""
        self.close()


# íŠ¹í™”ëœ Dask ì²˜ë¦¬ í•¨ìˆ˜ë“¤
def dask_factor_analysis(
    data_path: Union[str, pd.DataFrame],
    config: Optional[DaskConfig] = None
) -> ProcessingResult:
    """Daskë¥¼ ì‚¬ìš©í•œ íŒ©í„° ë¶„ì„"""
    processor = DaskProcessor(config)
    
    try:
        # ë°ì´í„° ë¡œë“œ
        ddf = processor.load_data(data_path)
        
        # íŒ©í„° ìƒì„± ì—°ì‚° ì •ì˜
        def generate_momentum_factors(df):
            df = df.copy()
            if 'close' in df.columns:
                df['momentum_5'] = df['close'].pct_change(5)
                df['momentum_20'] = df['close'].pct_change(20)
            return df
        
        def generate_volatility_factors(df):
            df = df.copy()
            if 'close' in df.columns:
                df['volatility_20'] = df['close'].pct_change().rolling(20).std()
            return df
        
        operations = [generate_momentum_factors, generate_volatility_factors]
        
        # ì²˜ë¦¬ ì‹¤í–‰
        result = processor.process_data(ddf, operations)
        
        return result
    
    finally:
        processor.close()


def dask_performance_calculation(
    data_path: Union[str, pd.DataFrame],
    config: Optional[DaskConfig] = None
) -> ProcessingResult:
    """Daskë¥¼ ì‚¬ìš©í•œ ì„±ëŠ¥ ê³„ì‚°"""
    processor = DaskProcessor(config)
    
    try:
        ddf = processor.load_data(data_path)
        
        def calculate_returns(df):
            df = df.copy()
            if 'close' in df.columns:
                df['returns'] = df['close'].pct_change()
                df['cumulative_returns'] = (1 + df['returns']).cumprod()
            return df
        
        def calculate_rolling_metrics(df):
            df = df.copy()
            if 'returns' in df.columns:
                df['rolling_sharpe'] = df['returns'].rolling(252).mean() / df['returns'].rolling(252).std() * np.sqrt(252)
                df['rolling_volatility'] = df['returns'].rolling(252).std() * np.sqrt(252)
            return df
        
        operations = [calculate_returns, calculate_rolling_metrics]
        result = processor.process_data(ddf, operations)
        
        return result
    
    finally:
        processor.close()


# Streamlit í†µí•©
def show_dask_dashboard():
    """Dask ëŒ€ì‹œë³´ë“œ í‘œì‹œ"""
    import streamlit as st
    
    st.subheader("ğŸ”§ Dask í´ëŸ¬ìŠ¤í„° ì •ë³´")
    
    if not DASK_AVAILABLE:
        st.error("Daskê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return
    
    # ì„ì‹œ í”„ë¡œì„¸ì„œë¡œ í´ëŸ¬ìŠ¤í„° ì •ë³´ í™•ì¸
    temp_processor = DaskProcessor()
    cluster_info = temp_processor.get_cluster_info()
    temp_processor.close()
    
    if cluster_info['status'] == 'active':
        col1, col2, col3 = st.columns(3)
        
        with col1:
            st.metric("ì›Œì»¤ ìˆ˜", cluster_info['n_workers'])
        
        with col2:
            st.metric("ì´ ì½”ì–´ ìˆ˜", cluster_info['total_cores'])
        
        with col3:
            memory_gb = cluster_info['total_memory'] / (1024**3) if cluster_info['total_memory'] else 0
            st.metric("ì´ ë©”ëª¨ë¦¬", f"{memory_gb:.1f} GB")
        
        # ëŒ€ì‹œë³´ë“œ ë§í¬
        if cluster_info.get('dashboard_link'):
            st.markdown(f"[Dask ëŒ€ì‹œë³´ë“œ ì—´ê¸°]({cluster_info['dashboard_link']})")
    
    elif cluster_info['status'] == 'no_cluster':
        st.info(f"í´ëŸ¬ìŠ¤í„° ì—†ìŒ - {cluster_info['backend']} ë°±ì—”ë“œ ì‚¬ìš© ì¤‘")
    
    else:
        st.error(f"í´ëŸ¬ìŠ¤í„° ì˜¤ë¥˜: {cluster_info.get('error', 'Unknown error')}")


def create_dask_config_ui() -> DaskConfig:
    """Dask ì„¤ì • UI ìƒì„±"""
    import streamlit as st
    
    st.subheader("âš™ï¸ Dask ì„¤ì •")
    
    col1, col2 = st.columns(2)
    
    with col1:
        backend = st.selectbox(
            "ë°±ì—”ë“œ",
            options=[b.value for b in DaskBackend],
            index=0
        )
        
        n_workers = st.number_input("ì›Œì»¤ ìˆ˜", min_value=1, max_value=16, value=4)
        
        threads_per_worker = st.number_input("ì›Œì»¤ë‹¹ ìŠ¤ë ˆë“œ", min_value=1, max_value=8, value=2)
    
    with col2:
        memory_limit = st.selectbox(
            "ë©”ëª¨ë¦¬ ì œí•œ",
            options=["1GB", "2GB", "4GB", "8GB"],
            index=1
        )
        
        chunk_size = st.selectbox(
            "ì²­í¬ í¬ê¸°",
            options=["50MB", "100MB", "200MB", "500MB"],
            index=1
        )
        
        compute_mode = st.selectbox(
            "ê³„ì‚° ëª¨ë“œ",
            options=[m.value for m in ComputeMode],
            index=0
        )
    
    return DaskConfig(
        backend=DaskBackend(backend),
        n_workers=n_workers,
        threads_per_worker=threads_per_worker,
        memory_limit=memory_limit,
        chunk_size=chunk_size,
        compute_mode=ComputeMode(compute_mode)
    )