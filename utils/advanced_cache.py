"""
ê³ ê¸‰ ìºì‹± ì „ëµ ì‹œìŠ¤í…œ
ë‹¤ì¸µ ìºì‹œ, ì§€ëŠ¥í˜• ë§Œë£Œ, ë¶„ì‚° ìºì‹œ, ìºì‹œ íˆíŠ¸ìœ¨ ìµœì í™”
"""

import os
import time
import threading
import pickle
import json
import hashlib
import lru
from datetime import datetime, timedelta
from typing import Dict, Any, List, Optional, Union, Callable, Tuple
from dataclasses import dataclass, field
from enum import Enum
import weakref
import gc
import psutil
from pathlib import Path
from collections import defaultdict, OrderedDict
import numpy as np
import pandas as pd

# Redis ê´€ë ¨ imports (ì„ íƒì )
try:
    import redis
    REDIS_AVAILABLE = True
except ImportError:
    REDIS_AVAILABLE = False

from utils.logger import get_logger, log_performance
from utils.error_handler import handle_error, ErrorCategory, ErrorSeverity


class CacheLevel(Enum):
    """ìºì‹œ ë ˆë²¨"""
    MEMORY = "memory"
    DISK = "disk"
    DISTRIBUTED = "distributed"
    PERSISTENT = "persistent"


class CachePolicy(Enum):
    """ìºì‹œ ì •ì±…"""
    LRU = "lru"
    LFU = "lfu"
    TTL = "ttl"
    ADAPTIVE = "adaptive"
    PREDICTIVE = "predictive"


class EvictionStrategy(Enum):
    """ì œê±° ì „ëµ"""
    SIZE_BASED = "size_based"
    TIME_BASED = "time_based"
    ACCESS_BASED = "access_based"
    PRIORITY_BASED = "priority_based"
    ML_BASED = "ml_based"


@dataclass
class CacheEntry:
    """ìºì‹œ ì—”íŠ¸ë¦¬"""
    key: str
    value: Any
    created_at: datetime
    last_accessed: datetime
    access_count: int
    size_bytes: int
    ttl: Optional[int] = None
    priority: int = 1
    cost: float = 1.0
    tags: List[str] = field(default_factory=list)
    dependency_keys: List[str] = field(default_factory=list)
    
    def is_expired(self) -> bool:
        """ë§Œë£Œ ì—¬ë¶€ í™•ì¸"""
        if self.ttl is None:
            return False
        return (datetime.now() - self.created_at).total_seconds() > self.ttl
    
    def update_access(self):
        """ì ‘ê·¼ ì •ë³´ ì—…ë°ì´íŠ¸"""
        self.last_accessed = datetime.now()
        self.access_count += 1


@dataclass
class CacheStats:
    """ìºì‹œ í†µê³„"""
    hits: int = 0
    misses: int = 0
    evictions: int = 0
    size_bytes: int = 0
    entry_count: int = 0
    avg_access_time_ms: float = 0
    hit_ratio: float = 0
    memory_usage_mb: float = 0
    last_reset: datetime = field(default_factory=datetime.now)
    
    def update_hit_ratio(self):
        """íˆíŠ¸ìœ¨ ì—…ë°ì´íŠ¸"""
        total = self.hits + self.misses
        self.hit_ratio = self.hits / total if total > 0 else 0


class IntelligentCache:
    """ì§€ëŠ¥í˜• ë‹¤ì¸µ ìºì‹œ ì‹œìŠ¤í…œ"""
    
    def __init__(
        self,
        max_memory_mb: int = 500,
        max_disk_mb: int = 2000,
        default_ttl: int = 3600,
        policy: CachePolicy = CachePolicy.ADAPTIVE,
        enable_distributed: bool = False,
        redis_url: Optional[str] = None
    ):
        """ì´ˆê¸°í™”"""
        self.logger = get_logger("intelligent_cache")
        self.max_memory_bytes = max_memory_mb * 1024 * 1024
        self.max_disk_bytes = max_disk_mb * 1024 * 1024
        self.default_ttl = default_ttl
        self.policy = policy
        
        # ìºì‹œ ì €ì¥ì†Œë“¤
        self.memory_cache: Dict[str, CacheEntry] = {}
        self.disk_cache_dir = Path(".cache/advanced")
        self.disk_cache_dir.mkdir(parents=True, exist_ok=True)
        
        # Redis ë¶„ì‚° ìºì‹œ
        self.redis_client = None
        if enable_distributed and REDIS_AVAILABLE and redis_url:
            try:
                self.redis_client = redis.from_url(redis_url)
                self.redis_client.ping()
                self.logger.info("Connected to Redis distributed cache")
            except Exception as e:
                self.logger.warning(f"Failed to connect to Redis: {e}")
                self.redis_client = None
        
        # í†µê³„ ë° ë©”íŠ¸ë¦­
        self.stats = CacheStats()
        
        # ì ‘ê·¼ íŒ¨í„´ ì¶”ì 
        self.access_patterns: Dict[str, List[datetime]] = defaultdict(list)
        self.prediction_model = None
        
        # LRU ìºì‹œ (ë©”ëª¨ë¦¬ ë ˆë²¨)
        self.lru_cache = lru.LRU(1000)  # ìµœëŒ€ 1000ê°œ ì—”íŠ¸ë¦¬
        
        # ìŠ¤ë ˆë“œ ì•ˆì „ì„±
        self.lock = threading.RLock()
        
        # ë°±ê·¸ë¼ìš´ë“œ ì‘ì—…
        self.cleanup_thread = None
        self.start_background_tasks()
        
        # ìºì‹œ ì›Œë°ì—… ìƒíƒœ
        self.warmup_status = {'completed': 0, 'total': 0, 'active': False}
    
    def start_background_tasks(self):
        """ë°±ê·¸ë¼ìš´ë“œ ì‘ì—… ì‹œì‘"""
        self.cleanup_thread = threading.Thread(
            target=self._cleanup_loop,
            name="CacheCleanup",
            daemon=True
        )
        self.cleanup_thread.start()
    
    def get(self, key: str, level_preference: Optional[CacheLevel] = None) -> Optional[Any]:
        """ìºì‹œì—ì„œ ê°’ ì¡°íšŒ"""
        start_time = time.time()
        
        with self.lock:
            try:
                # 1. ë©”ëª¨ë¦¬ ìºì‹œ í™•ì¸
                if key in self.memory_cache:
                    entry = self.memory_cache[key]
                    if not entry.is_expired():
                        entry.update_access()
                        self._record_access_pattern(key)
                        self.stats.hits += 1
                        self._update_access_time(start_time)
                        self.logger.debug(f"Memory cache hit: {key}")
                        return entry.value
                    else:
                        # ë§Œë£Œëœ ì—”íŠ¸ë¦¬ ì œê±°
                        del self.memory_cache[key]
                
                # 2. ë””ìŠ¤í¬ ìºì‹œ í™•ì¸
                disk_entry = self._load_from_disk(key)
                if disk_entry and not disk_entry.is_expired():
                    # ë©”ëª¨ë¦¬ë¡œ ìŠ¹ê²©
                    self._promote_to_memory(disk_entry)
                    disk_entry.update_access()
                    self._record_access_pattern(key)
                    self.stats.hits += 1
                    self._update_access_time(start_time)
                    self.logger.debug(f"Disk cache hit: {key}")
                    return disk_entry.value
                
                # 3. ë¶„ì‚° ìºì‹œ í™•ì¸
                if self.redis_client:
                    distributed_value = self._load_from_redis(key)
                    if distributed_value is not None:
                        # ë¡œì»¬ ìºì‹œë¡œ ë³µì‚¬
                        self.set(key, distributed_value, cache_locally=True)
                        self.stats.hits += 1
                        self._update_access_time(start_time)
                        self.logger.debug(f"Distributed cache hit: {key}")
                        return distributed_value
                
                # ìºì‹œ ë¯¸ìŠ¤
                self.stats.misses += 1
                self._update_access_time(start_time)
                return None
                
            except Exception as e:
                self.logger.error(f"Cache get failed for key {key}: {e}")
                self.stats.misses += 1
                return None
    
    def set(
        self,
        key: str,
        value: Any,
        ttl: Optional[int] = None,
        priority: int = 1,
        tags: Optional[List[str]] = None,
        level: Optional[CacheLevel] = None,
        cache_locally: bool = True
    ) -> bool:
        """ìºì‹œì— ê°’ ì €ì¥"""
        with self.lock:
            try:
                ttl = ttl or self.default_ttl
                tags = tags or []
                
                # ê°’ í¬ê¸° ê³„ì‚°
                size_bytes = self._calculate_size(value)
                
                # ìºì‹œ ì—”íŠ¸ë¦¬ ìƒì„±
                entry = CacheEntry(
                    key=key,
                    value=value,
                    created_at=datetime.now(),
                    last_accessed=datetime.now(),
                    access_count=1,
                    size_bytes=size_bytes,
                    ttl=ttl,
                    priority=priority,
                    tags=tags
                )
                
                # ì €ì¥ ë ˆë²¨ ê²°ì •
                target_level = self._determine_storage_level(entry, level)
                
                if target_level == CacheLevel.MEMORY and cache_locally:
                    self._store_in_memory(entry)
                elif target_level == CacheLevel.DISK:
                    self._store_on_disk(entry)
                
                # ë¶„ì‚° ìºì‹œì—ë„ ì €ì¥ (ì¡°ê±´ë¶€)
                if self.redis_client and (priority >= 3 or size_bytes < 1024 * 100):  # 100KB ë¯¸ë§Œ ë˜ëŠ” ë†’ì€ ìš°ì„ ìˆœìœ„
                    self._store_in_redis(key, value, ttl)
                
                self.stats.entry_count += 1
                self.stats.size_bytes += size_bytes
                
                return True
                
            except Exception as e:
                self.logger.error(f"Cache set failed for key {key}: {e}")
                handle_error(
                    e,
                    ErrorCategory.SYSTEM,
                    ErrorSeverity.MEDIUM,
                    context={'key': key, 'size_bytes': size_bytes}
                )
                return False
    
    def invalidate(self, key: str, propagate: bool = True) -> bool:
        """ìºì‹œ ë¬´íš¨í™”"""
        with self.lock:
            try:
                removed = False
                
                # ë©”ëª¨ë¦¬ì—ì„œ ì œê±°
                if key in self.memory_cache:
                    entry = self.memory_cache[key]
                    self.stats.size_bytes -= entry.size_bytes
                    del self.memory_cache[key]
                    removed = True
                
                # ë””ìŠ¤í¬ì—ì„œ ì œê±°
                disk_path = self.disk_cache_dir / f"{key}.pkl"
                if disk_path.exists():
                    disk_path.unlink()
                    removed = True
                
                # ë¶„ì‚° ìºì‹œì—ì„œ ì œê±°
                if self.redis_client and propagate:
                    self.redis_client.delete(key)
                    removed = True
                
                if removed:
                    self.stats.entry_count -= 1
                    self.logger.debug(f"Invalidated cache key: {key}")
                
                return removed
                
            except Exception as e:
                self.logger.error(f"Cache invalidation failed for key {key}: {e}")
                return False
    
    def invalidate_by_tags(self, tags: List[str]) -> int:
        """íƒœê·¸ë¡œ ìºì‹œ ë¬´íš¨í™”"""
        with self.lock:
            invalidated_count = 0
            
            # ë©”ëª¨ë¦¬ ìºì‹œì—ì„œ íƒœê·¸ ê¸°ë°˜ ë¬´íš¨í™”
            keys_to_remove = []
            for key, entry in self.memory_cache.items():
                if any(tag in entry.tags for tag in tags):
                    keys_to_remove.append(key)
            
            for key in keys_to_remove:
                if self.invalidate(key):
                    invalidated_count += 1
            
            # ë””ìŠ¤í¬ ìºì‹œëŠ” ë©”íƒ€ë°ì´í„° íŒŒì¼ ìŠ¤ìº” í•„ìš” (ì„±ëŠ¥ìƒ ìƒëµ)
            
            self.logger.info(f"Invalidated {invalidated_count} cache entries by tags: {tags}")
            return invalidated_count
    
    def warm_up(self, warm_up_functions: List[Callable[[], Dict[str, Any]]]) -> Dict[str, Any]:
        """ìºì‹œ ì›Œë°ì—…"""
        self.logger.info("Starting cache warm-up")
        start_time = time.time()
        
        self.warmup_status = {
            'completed': 0,
            'total': len(warm_up_functions),
            'active': True,
            'start_time': start_time
        }
        
        warmed_keys = []
        failed_functions = []
        
        for i, warm_up_func in enumerate(warm_up_functions):
            try:
                # ì›Œë°ì—… í•¨ìˆ˜ ì‹¤í–‰
                result = warm_up_func()
                
                if isinstance(result, dict):
                    for key, value in result.items():
                        if self.set(key, value, priority=2):  # ì¤‘ê°„ ìš°ì„ ìˆœìœ„
                            warmed_keys.append(key)
                
                self.warmup_status['completed'] = i + 1
                
            except Exception as e:
                self.logger.error(f"Warm-up function failed: {e}")
                failed_functions.append(str(e))
        
        self.warmup_status['active'] = False
        elapsed_time = time.time() - start_time
        
        self.logger.info(f"Cache warm-up completed: {len(warmed_keys)} keys in {elapsed_time:.2f}s")
        
        return {
            'warmed_keys': warmed_keys,
            'failed_functions': failed_functions,
            'elapsed_time': elapsed_time,
            'success_rate': len(warmed_keys) / max(1, len(warm_up_functions))
        }
    
    def get_predictive_suggestions(self, current_key: str) -> List[str]:
        """ì˜ˆì¸¡ì  ìºì‹œ ì œì•ˆ"""
        if self.policy != CachePolicy.PREDICTIVE:
            return []
        
        suggestions = []
        
        try:
            # í˜„ì¬ í‚¤ì˜ ì ‘ê·¼ íŒ¨í„´ ë¶„ì„
            if current_key in self.access_patterns:
                current_pattern = self.access_patterns[current_key]
                
                # ë¹„ìŠ·í•œ íŒ¨í„´ì˜ í‚¤ ì°¾ê¸°
                for other_key, other_pattern in self.access_patterns.items():
                    if other_key != current_key and len(other_pattern) > 5:
                        # ê°„ë‹¨í•œ íŒ¨í„´ ìœ ì‚¬ë„ ê³„ì‚°
                        similarity = self._calculate_pattern_similarity(current_pattern, other_pattern)
                        if similarity > 0.7:
                            suggestions.append(other_key)
            
            return suggestions[:5]  # ìƒìœ„ 5ê°œë§Œ ë°˜í™˜
            
        except Exception as e:
            self.logger.error(f"Predictive suggestions failed: {e}")
            return []
    
    def optimize_cache_placement(self):
        """ìºì‹œ ë°°ì¹˜ ìµœì í™”"""
        with self.lock:
            try:
                self.logger.info("Starting cache placement optimization")
                
                # ì ‘ê·¼ ë¹ˆë„ ë¶„ì„
                access_frequency = {}
                for key, entry in self.memory_cache.items():
                    access_frequency[key] = entry.access_count
                
                # ìƒìœ„ 20% -> ë©”ëª¨ë¦¬ ìœ ì§€
                # ì¤‘ê°„ 60% -> ë””ìŠ¤í¬ë¡œ ì´ë™
                # í•˜ìœ„ 20% -> ì œê±° ê³ ë ¤
                
                sorted_keys = sorted(access_frequency.keys(), key=lambda k: access_frequency[k], reverse=True)
                
                total_keys = len(sorted_keys)
                high_freq_count = int(total_keys * 0.2)
                medium_freq_count = int(total_keys * 0.6)
                
                # ì¤‘ê°„ ë¹ˆë„ í‚¤ë“¤ì„ ë””ìŠ¤í¬ë¡œ ì´ë™
                for key in sorted_keys[high_freq_count:high_freq_count + medium_freq_count]:
                    if key in self.memory_cache:
                        entry = self.memory_cache[key]
                        self._store_on_disk(entry)
                        del self.memory_cache[key]
                
                # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì—…ë°ì´íŠ¸
                self._update_memory_stats()
                
                self.logger.info("Cache placement optimization completed")
                
            except Exception as e:
                self.logger.error(f"Cache optimization failed: {e}")
    
    def _determine_storage_level(self, entry: CacheEntry, preferred_level: Optional[CacheLevel]) -> CacheLevel:
        """ì €ì¥ ë ˆë²¨ ê²°ì •"""
        if preferred_level:
            return preferred_level
        
        # ì ì‘ì  ê²°ì •
        if self.policy == CachePolicy.ADAPTIVE:
            # í¬ê¸° ê¸°ë°˜
            if entry.size_bytes > 10 * 1024 * 1024:  # 10MB ì´ìƒ
                return CacheLevel.DISK
            
            # ìš°ì„ ìˆœìœ„ ê¸°ë°˜
            if entry.priority >= 4:
                return CacheLevel.MEMORY
            
            # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê¸°ë°˜
            memory_usage_ratio = self.stats.size_bytes / self.max_memory_bytes
            if memory_usage_ratio > 0.8:
                return CacheLevel.DISK
            
            return CacheLevel.MEMORY
        
        return CacheLevel.MEMORY
    
    def _store_in_memory(self, entry: CacheEntry):
        """ë©”ëª¨ë¦¬ì— ì €ì¥"""
        # ë©”ëª¨ë¦¬ ì œí•œ í™•ì¸
        if self.stats.size_bytes + entry.size_bytes > self.max_memory_bytes:
            self._evict_from_memory()
        
        self.memory_cache[entry.key] = entry
        self.lru_cache[entry.key] = entry.value
    
    def _store_on_disk(self, entry: CacheEntry):
        """ë””ìŠ¤í¬ì— ì €ì¥"""
        try:
            cache_file = self.disk_cache_dir / f"{entry.key}.pkl"
            meta_file = self.disk_cache_dir / f"{entry.key}.meta"
            
            # ë°ì´í„° ì €ì¥
            with open(cache_file, 'wb') as f:
                pickle.dump(entry.value, f)
            
            # ë©”íƒ€ë°ì´í„° ì €ì¥
            meta_data = {
                'created_at': entry.created_at.isoformat(),
                'ttl': entry.ttl,
                'size_bytes': entry.size_bytes,
                'priority': entry.priority,
                'tags': entry.tags
            }
            
            with open(meta_file, 'w') as f:
                json.dump(meta_data, f)
            
        except Exception as e:
            self.logger.error(f"Disk storage failed for key {entry.key}: {e}")
    
    def _store_in_redis(self, key: str, value: Any, ttl: int):
        """Redisì— ì €ì¥"""
        try:
            serialized_value = pickle.dumps(value)
            self.redis_client.setex(key, ttl, serialized_value)
        except Exception as e:
            self.logger.error(f"Redis storage failed for key {key}: {e}")
    
    def _load_from_disk(self, key: str) -> Optional[CacheEntry]:
        """ë””ìŠ¤í¬ì—ì„œ ë¡œë“œ"""
        try:
            cache_file = self.disk_cache_dir / f"{key}.pkl"
            meta_file = self.disk_cache_dir / f"{key}.meta"
            
            if not cache_file.exists() or not meta_file.exists():
                return None
            
            # ë©”íƒ€ë°ì´í„° ë¡œë“œ
            with open(meta_file, 'r') as f:
                meta_data = json.load(f)
            
            # ë§Œë£Œ í™•ì¸
            created_at = datetime.fromisoformat(meta_data['created_at'])
            ttl = meta_data.get('ttl')
            if ttl and (datetime.now() - created_at).total_seconds() > ttl:
                # ë§Œë£Œëœ íŒŒì¼ ì‚­ì œ
                cache_file.unlink(missing_ok=True)
                meta_file.unlink(missing_ok=True)
                return None
            
            # ë°ì´í„° ë¡œë“œ
            with open(cache_file, 'rb') as f:
                value = pickle.load(f)
            
            # CacheEntry ì¬êµ¬ì„±
            entry = CacheEntry(
                key=key,
                value=value,
                created_at=created_at,
                last_accessed=datetime.now(),
                access_count=1,
                size_bytes=meta_data.get('size_bytes', 0),
                ttl=ttl,
                priority=meta_data.get('priority', 1),
                tags=meta_data.get('tags', [])
            )
            
            return entry
            
        except Exception as e:
            self.logger.error(f"Disk load failed for key {key}: {e}")
            return None
    
    def _load_from_redis(self, key: str) -> Optional[Any]:
        """Redisì—ì„œ ë¡œë“œ"""
        try:
            serialized_value = self.redis_client.get(key)
            if serialized_value:
                return pickle.loads(serialized_value)
            return None
        except Exception as e:
            self.logger.error(f"Redis load failed for key {key}: {e}")
            return None
    
    def _promote_to_memory(self, entry: CacheEntry):
        """ë©”ëª¨ë¦¬ë¡œ ìŠ¹ê²©"""
        self._store_in_memory(entry)
        
        # ë””ìŠ¤í¬ì—ì„œ ì œê±°
        cache_file = self.disk_cache_dir / f"{entry.key}.pkl"
        meta_file = self.disk_cache_dir / f"{entry.key}.meta"
        cache_file.unlink(missing_ok=True)
        meta_file.unlink(missing_ok=True)
    
    def _evict_from_memory(self):
        """ë©”ëª¨ë¦¬ì—ì„œ ì œê±°"""
        if not self.memory_cache:
            return
        
        # ì •ì±…ë³„ ì œê±° ì „ëµ
        if self.policy == CachePolicy.LRU:
            # ê°€ì¥ ì˜¤ë˜ ì‚¬ìš©ë˜ì§€ ì•Šì€ í•­ëª© ì œê±°
            oldest_key = min(
                self.memory_cache.keys(),
                key=lambda k: self.memory_cache[k].last_accessed
            )
            entry = self.memory_cache[oldest_key]
            
        elif self.policy == CachePolicy.LFU:
            # ê°€ì¥ ì ê²Œ ì‚¬ìš©ëœ í•­ëª© ì œê±°
            least_used_key = min(
                self.memory_cache.keys(),
                key=lambda k: self.memory_cache[k].access_count
            )
            entry = self.memory_cache[least_used_key]
            
        else:
            # ê¸°ë³¸: ê°€ì¥ í° í•­ëª© ì œê±°
            largest_key = max(
                self.memory_cache.keys(),
                key=lambda k: self.memory_cache[k].size_bytes
            )
            entry = self.memory_cache[largest_key]
        
        # ë””ìŠ¤í¬ë¡œ ì´ë™ (ìš°ì„ ìˆœìœ„ê°€ ë†’ì€ ê²½ìš°)
        if entry.priority >= 2:
            self._store_on_disk(entry)
        
        # ë©”ëª¨ë¦¬ì—ì„œ ì œê±°
        self.stats.size_bytes -= entry.size_bytes
        del self.memory_cache[entry.key]
        self.stats.evictions += 1
        
        self.logger.debug(f"Evicted from memory: {entry.key}")
    
    def _calculate_size(self, value: Any) -> int:
        """ê°ì²´ í¬ê¸° ê³„ì‚°"""
        try:
            if isinstance(value, (str, int, float, bool)):
                return len(str(value).encode())
            elif isinstance(value, (list, tuple, dict)):
                return len(pickle.dumps(value))
            elif isinstance(value, pd.DataFrame):
                return value.memory_usage(deep=True).sum()
            elif isinstance(value, np.ndarray):
                return value.nbytes
            else:
                return len(pickle.dumps(value))
        except Exception:
            return 1024  # ê¸°ë³¸ê°’
    
    def _record_access_pattern(self, key: str):
        """ì ‘ê·¼ íŒ¨í„´ ê¸°ë¡"""
        current_time = datetime.now()
        self.access_patterns[key].append(current_time)
        
        # ìµœê·¼ 100ê°œ ì ‘ê·¼ë§Œ ìœ ì§€
        if len(self.access_patterns[key]) > 100:
            self.access_patterns[key] = self.access_patterns[key][-100:]
    
    def _calculate_pattern_similarity(self, pattern1: List[datetime], pattern2: List[datetime]) -> float:
        """íŒ¨í„´ ìœ ì‚¬ë„ ê³„ì‚°"""
        try:
            # ê°„ë‹¨í•œ ì ‘ê·¼ ê°„ê²© ê¸°ë°˜ ìœ ì‚¬ë„
            if len(pattern1) < 3 or len(pattern2) < 3:
                return 0.0
            
            intervals1 = [(pattern1[i] - pattern1[i-1]).total_seconds() 
                         for i in range(1, min(len(pattern1), 10))]
            intervals2 = [(pattern2[i] - pattern2[i-1]).total_seconds() 
                         for i in range(1, min(len(pattern2), 10))]
            
            # í‰ê·  ê°„ê²© ë¹„êµ
            avg1 = sum(intervals1) / len(intervals1)
            avg2 = sum(intervals2) / len(intervals2)
            
            # ìœ ì‚¬ë„ ê³„ì‚° (ì°¨ì´ê°€ ì ì„ìˆ˜ë¡ ë†’ì€ ì ìˆ˜)
            max_diff = max(avg1, avg2, 1)
            similarity = 1.0 - abs(avg1 - avg2) / max_diff
            
            return max(0.0, similarity)
            
        except Exception:
            return 0.0
    
    def _update_access_time(self, start_time: float):
        """ì ‘ê·¼ ì‹œê°„ ì—…ë°ì´íŠ¸"""
        access_time_ms = (time.time() - start_time) * 1000
        
        # ì§€ìˆ˜ ì´ë™ í‰ê· ìœ¼ë¡œ ì—…ë°ì´íŠ¸
        alpha = 0.1
        self.stats.avg_access_time_ms = (
            alpha * access_time_ms + (1 - alpha) * self.stats.avg_access_time_ms
        )
    
    def _update_memory_stats(self):
        """ë©”ëª¨ë¦¬ í†µê³„ ì—…ë°ì´íŠ¸"""
        self.stats.size_bytes = sum(entry.size_bytes for entry in self.memory_cache.values())
        self.stats.entry_count = len(self.memory_cache)
        self.stats.update_hit_ratio()
        
        # ì‹œìŠ¤í…œ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
        process = psutil.Process()
        self.stats.memory_usage_mb = process.memory_info().rss / 1024 / 1024
    
    def _cleanup_loop(self):
        """ë°±ê·¸ë¼ìš´ë“œ ì •ë¦¬ ë£¨í”„"""
        while True:
            try:
                time.sleep(300)  # 5ë¶„ë§ˆë‹¤ ì‹¤í–‰
                
                with self.lock:
                    # ë§Œë£Œëœ ì—”íŠ¸ë¦¬ ì •ë¦¬
                    expired_keys = [
                        key for key, entry in self.memory_cache.items()
                        if entry.is_expired()
                    ]
                    
                    for key in expired_keys:
                        self.invalidate(key)
                    
                    # ë””ìŠ¤í¬ ìºì‹œ ì •ë¦¬
                    self._cleanup_disk_cache()
                    
                    # í†µê³„ ì—…ë°ì´íŠ¸
                    self._update_memory_stats()
                    
                    if expired_keys:
                        self.logger.info(f"Cleaned up {len(expired_keys)} expired cache entries")
                
            except Exception as e:
                self.logger.error(f"Cache cleanup error: {e}")
    
    def _cleanup_disk_cache(self):
        """ë””ìŠ¤í¬ ìºì‹œ ì •ë¦¬"""
        try:
            for cache_file in self.disk_cache_dir.glob("*.pkl"):
                meta_file = cache_file.with_suffix('.meta')
                
                if meta_file.exists():
                    try:
                        with open(meta_file, 'r') as f:
                            meta_data = json.load(f)
                        
                        created_at = datetime.fromisoformat(meta_data['created_at'])
                        ttl = meta_data.get('ttl')
                        
                        if ttl and (datetime.now() - created_at).total_seconds() > ttl:
                            cache_file.unlink(missing_ok=True)
                            meta_file.unlink(missing_ok=True)
                    
                    except Exception:
                        # ì†ìƒëœ ë©”íƒ€ë°ì´í„° íŒŒì¼ ì œê±°
                        cache_file.unlink(missing_ok=True)
                        meta_file.unlink(missing_ok=True)
        
        except Exception as e:
            self.logger.error(f"Disk cache cleanup failed: {e}")
    
    def get_stats(self) -> Dict[str, Any]:
        """ìºì‹œ í†µê³„ ì¡°íšŒ"""
        with self.lock:
            self._update_memory_stats()
            
            return {
                'hits': self.stats.hits,
                'misses': self.stats.misses,
                'hit_ratio': self.stats.hit_ratio,
                'evictions': self.stats.evictions,
                'memory_entries': len(self.memory_cache),
                'memory_size_mb': self.stats.size_bytes / 1024 / 1024,
                'memory_usage_mb': self.stats.memory_usage_mb,
                'avg_access_time_ms': self.stats.avg_access_time_ms,
                'disk_cache_files': len(list(self.disk_cache_dir.glob("*.pkl"))),
                'redis_connected': self.redis_client is not None,
                'policy': self.policy.value,
                'warmup_status': self.warmup_status
            }
    
    def export_metrics(self) -> Dict[str, Any]:
        """ë©”íŠ¸ë¦­ ë‚´ë³´ë‚´ê¸°"""
        stats = self.get_stats()
        
        return {
            'timestamp': datetime.now().isoformat(),
            'cache_performance': {
                'hit_ratio': stats['hit_ratio'],
                'avg_access_time_ms': stats['avg_access_time_ms'],
                'memory_efficiency': stats['memory_size_mb'] / max(1, stats['memory_entries'])
            },
            'resource_usage': {
                'memory_usage_mb': stats['memory_usage_mb'],
                'disk_usage_files': stats['disk_cache_files']
            },
            'cache_stats': stats
        }
    
    def clear_all(self):
        """ëª¨ë“  ìºì‹œ ì‚­ì œ"""
        with self.lock:
            # ë©”ëª¨ë¦¬ ìºì‹œ ì‚­ì œ
            self.memory_cache.clear()
            self.lru_cache.clear()
            
            # ë””ìŠ¤í¬ ìºì‹œ ì‚­ì œ
            for cache_file in self.disk_cache_dir.glob("*"):
                cache_file.unlink(missing_ok=True)
            
            # Redis ìºì‹œ ì‚­ì œ (ì£¼ì˜: ì „ì²´ Redis DB ì‚­ì œ)
            if self.redis_client:
                try:
                    self.redis_client.flushdb()
                except Exception as e:
                    self.logger.error(f"Redis flush failed: {e}")
            
            # í†µê³„ ì´ˆê¸°í™”
            self.stats = CacheStats()
            self.access_patterns.clear()
            
            self.logger.info("All caches cleared")


# ê¸€ë¡œë²Œ ê³ ê¸‰ ìºì‹œ ì¸ìŠ¤í„´ìŠ¤
advanced_cache = IntelligentCache(
    max_memory_mb=int(os.getenv('CACHE_MEMORY_MB', '500')),
    max_disk_mb=int(os.getenv('CACHE_DISK_MB', '2000')),
    enable_distributed=os.getenv('REDIS_URL') is not None,
    redis_url=os.getenv('REDIS_URL')
)


# ìºì‹œ ë°ì½”ë ˆì´í„°ë“¤
def intelligent_cache(
    ttl: int = 3600,
    priority: int = 1,
    tags: Optional[List[str]] = None,
    level: Optional[CacheLevel] = None
):
    """ì§€ëŠ¥í˜• ìºì‹œ ë°ì½”ë ˆì´í„°"""
    def decorator(func: Callable):
        def wrapper(*args, **kwargs):
            # ìºì‹œ í‚¤ ìƒì„±
            key_data = {
                'func': func.__name__,
                'module': func.__module__,
                'args': str(args)[:200],  # ê¸¸ì´ ì œí•œ
                'kwargs': str(sorted(kwargs.items()))[:200]
            }
            cache_key = hashlib.md5(str(key_data).encode()).hexdigest()
            
            # ìºì‹œì—ì„œ ì¡°íšŒ
            cached_result = advanced_cache.get(cache_key)
            if cached_result is not None:
                return cached_result
            
            # í•¨ìˆ˜ ì‹¤í–‰
            start_time = time.time()
            result = func(*args, **kwargs)
            execution_time = time.time() - start_time
            
            # ì‹¤í–‰ ì‹œê°„ì´ ê¸´ ê²½ìš° ë†’ì€ ìš°ì„ ìˆœìœ„ë¡œ ìºì‹œ
            cache_priority = priority
            if execution_time > 1.0:  # 1ì´ˆ ì´ìƒ
                cache_priority = max(priority, 3)
            
            # ìºì‹œì— ì €ì¥
            advanced_cache.set(
                cache_key,
                result,
                ttl=ttl,
                priority=cache_priority,
                tags=tags,
                level=level
            )
            
            return result
        
        return wrapper
    return decorator


# Streamlit í†µí•©
def show_advanced_cache_dashboard():
    """ê³ ê¸‰ ìºì‹œ ëŒ€ì‹œë³´ë“œ í‘œì‹œ"""
    import streamlit as st
    
    st.subheader("ğŸ§  ì§€ëŠ¥í˜• ìºì‹œ ì‹œìŠ¤í…œ")
    
    # ìºì‹œ í†µê³„
    stats = advanced_cache.get_stats()
    
    # ë©”ì¸ ë©”íŠ¸ë¦­
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.metric("íˆíŠ¸ìœ¨", f"{stats['hit_ratio']*100:.1f}%")
    
    with col2:
        st.metric("í‰ê·  ì ‘ê·¼ ì‹œê°„", f"{stats['avg_access_time_ms']:.1f}ms")
    
    with col3:
        st.metric("ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰", f"{stats['memory_size_mb']:.1f}MB")
    
    with col4:
        st.metric("ì´ ì—”íŠ¸ë¦¬", stats['memory_entries'])
    
    # ìƒì„¸ í†µê³„
    st.subheader("ğŸ“Š ìƒì„¸ í†µê³„")
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.metric("ìºì‹œ íˆíŠ¸", stats['hits'])
        st.metric("ìºì‹œ ë¯¸ìŠ¤", stats['misses'])
        st.metric("ì œê±°ëœ í•­ëª©", stats['evictions'])
    
    with col2:
        st.metric("ë””ìŠ¤í¬ ìºì‹œ íŒŒì¼", stats['disk_cache_files'])
        st.metric("Redis ì—°ê²°", "í™œì„±" if stats['redis_connected'] else "ë¹„í™œì„±")
        st.metric("ìºì‹œ ì •ì±…", stats['policy'].upper())
    
    # ì›Œë°ì—… ìƒíƒœ
    warmup = stats['warmup_status']
    if warmup['active']:
        st.subheader("ğŸ”¥ ìºì‹œ ì›Œë°ì—… ì§„í–‰ ì¤‘")
        progress = warmup['completed'] / max(1, warmup['total'])
        st.progress(progress)
        st.text(f"ì§„í–‰ë¥ : {warmup['completed']}/{warmup['total']}")
    
    # ì œì–´ ë²„íŠ¼
    st.subheader("ğŸ›ï¸ ìºì‹œ ê´€ë¦¬")
    
    col1, col2, col3 = st.columns(3)
    
    with col1:
        if st.button("ìºì‹œ ìµœì í™”"):
            advanced_cache.optimize_cache_placement()
            st.success("ìºì‹œ ìµœì í™”ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
            st.experimental_rerun()
    
    with col2:
        if st.button("í†µê³„ ì´ˆê¸°í™”"):
            advanced_cache.stats = CacheStats()
            st.success("í†µê³„ê°€ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")
            st.experimental_rerun()
    
    with col3:
        if st.button("âš ï¸ ì „ì²´ ìºì‹œ ì‚­ì œ"):
            advanced_cache.clear_all()
            st.success("ëª¨ë“  ìºì‹œê°€ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤.")
            st.experimental_rerun()
    
    # ë©”íŠ¸ë¦­ ë‚´ë³´ë‚´ê¸°
    if st.button("ë©”íŠ¸ë¦­ ë‚´ë³´ë‚´ê¸°"):
        metrics = advanced_cache.export_metrics()
        st.json(metrics)


# ìºì‹œ ì›Œë°ì—… í•¨ìˆ˜ë“¤
def create_cache_warmup_functions() -> List[Callable]:
    """ìºì‹œ ì›Œë°ì—… í•¨ìˆ˜ë“¤ ìƒì„±"""
    warmup_functions = []
    
    def warmup_common_calculations():
        """ê³µí†µ ê³„ì‚° ì›Œë°ì—…"""
        import numpy as np
        return {
            'pi': np.pi,
            'e': np.e,
            'sqrt2': np.sqrt(2),
            'common_dates': pd.date_range('2020-01-01', '2023-12-31', freq='D').tolist()
        }
    
    def warmup_sample_data():
        """ìƒ˜í”Œ ë°ì´í„° ì›Œë°ì—…"""
        return {
            'sample_prices': np.random.randn(1000) * 10 + 100,
            'sample_returns': np.random.randn(1000) * 0.02,
            'sample_volumes': np.random.randint(1000000, 10000000, 1000)
        }
    
    warmup_functions.extend([
        warmup_common_calculations,
        warmup_sample_data
    ])
    
    return warmup_functions